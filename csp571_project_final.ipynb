{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "c49bd141"
   },
   "source": [
    "# CSP 571 Project by:-\n",
    "# Aman Kumar A20538809\n",
    "# Prachi Kotadia A20549927\n",
    "# Vinay Jaikumar Gupta A20554266\n",
    "# R Prem Swaroopa Nanda A20547712\n",
    "# Ganapathi Subramaniam A20536260\n",
    "# Title:- High-Efficiency Neural Networks for Mobile Vision Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90ef2381",
    "outputId": "52abf210-0aa8-4c1a-837d-6e2743dd5436"
   },
   "outputs": [],
   "source": [
    "# Step 1: Manually download and extract PASCAL VOC 2007 in Colab\n",
    "import os\n",
    "\n",
    "manual_dir = '/root/tensorflow_datasets/downloads/manual'\n",
    "os.makedirs(manual_dir, exist_ok=True)\n",
    "\n",
    "!wget -nc http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar -P {manual_dir}\n",
    "!wget -nc http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar -P {manual_dir}\n",
    "!tar -xf {manual_dir}/VOCtrainval_06-Nov-2007.tar -C {manual_dir}\n",
    "!tar -xf {manual_dir}/VOCtest_06-Nov-2007.tar -C {manual_dir}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "30eca2a4"
   },
   "source": [
    "# Importing the required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "09bfd554"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, ReLU, BatchNormalization, GlobalAveragePooling2D, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "4c2175be"
   },
   "source": [
    "# Inverted Residual Block with Linear Bottleneck Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "df87169c"
   },
   "outputs": [],
   "source": [
    "# Inverted residual block with linear bottleneck\n",
    "def inverted_residual_block(x, filters, stride, expansion):\n",
    "    shortcut = x\n",
    "    expanded_filters = tf.keras.backend.int_shape(x)[-1] * expansion\n",
    "\n",
    "    # The Expansion layer\n",
    "    if expansion != 1:\n",
    "        x = Conv2D(expanded_filters, kernel_size=1, padding='same', use_bias=False)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU(6.)(x)\n",
    "\n",
    "    # Depthwise Convolution\n",
    "    x = DepthwiseConv2D(kernel_size=3, strides=stride, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU(6.)(x)\n",
    "\n",
    "    # Below is the Projection layer which reduces channels back to `filters`\n",
    "    x = Conv2D(filters, kernel_size=1, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # This below is the Residual connection to check if dimensions match\n",
    "    if stride == 1 and tf.keras.backend.int_shape(shortcut)[-1] == filters:\n",
    "        x = Add()([x, shortcut])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "c4e9335b"
   },
   "source": [
    "# MobileNetV2 Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "07179b60"
   },
   "outputs": [],
   "source": [
    "# Here we have defined the Mobile Net V2 architecture\n",
    "def MobileNetV2(input_shape=(224, 224, 3), num_classes=20):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Initial convolution layer\n",
    "    x = Conv2D(32, kernel_size=3, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU(6.)(x)\n",
    "\n",
    "    # This is the block configurations for Mobile net V2\n",
    "    inverted_residual_setting = [\n",
    "        (1, 16, 1, 1),\n",
    "        (6, 24, 2, 2),\n",
    "        (6, 32, 2, 3),\n",
    "        (6, 64, 2, 4),\n",
    "        (6, 96, 1, 3),\n",
    "        (6, 160, 2, 3),\n",
    "        (6, 320, 1, 1),\n",
    "    ]\n",
    "\n",
    "    for t, c, s, n in inverted_residual_setting:\n",
    "        for i in range(n):\n",
    "            x = inverted_residual_block(x, c, s if i == 0 else 1, t)\n",
    "\n",
    "    # Final layers of the Mobile net v2 architecture\n",
    "    x = Conv2D(1280, kernel_size=1, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU(6.)(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.load_weights('/content/custom_mobilenetv2_imagenet_weights_for_20_classes.weights.h5')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "f2ce7aff"
   },
   "source": [
    "# MobileNetV2 Model Creation and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "tGICnwIRiZVk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot raw images with their original label IDs\n",
    "def plot_raw_dataset(dataset, num_images=9):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, example in enumerate(dataset.take(num_images)):\n",
    "        image = example['image']\n",
    "        label_ids = example['objects']['label']\n",
    "\n",
    "        # Convert TensorFlow tensor to numpy and plot\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(image.numpy())\n",
    "        label_text = \", \".join(str(label_id.numpy()) for label_id in label_ids)\n",
    "        plt.title(f\"Raw labels: {label_text}\", fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "011d2213",
    "outputId": "3f45f7ff-9eeb-42e1-88f3-51bfd15e44de"
   },
   "outputs": [],
   "source": [
    "# Model instantiation for 20 classes as per PASCAL VOC 2007 dataset\n",
    "model = MobileNetV2(input_shape=(224, 224, 3), num_classes=20)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "referenced_widgets": [
      "0d6f9cc997ad4c51a0ba59470f01b036",
      "d8cd325304374cd0af37b5c5e2bec6f8",
      "b6895c5e0ed042dbbf1d4f32d4f12671",
      "d5535bff974d4f13a50e5680f873d923",
      "f0104aae4efd494c8dd462ecdf75676a",
      "15e0e73c46cb446f9549678b6bc660e7",
      "8db7113b4b2b48639cdf93a22083dfc7",
      "3cdac68d1d8d45eaaef6fe92b7a3d06b",
      "696c2b217ce34e0aa69612ef63691a81",
      "559dc4cd11974ab195aea85beebe64f1",
      "3bb737c660fb4cc8a1dff568125b486d",
      "8cc4e771f5514f31824a27dd489fa51a",
      "4dc423dfc9b24e2796f0eb554ac5b57b",
      "cfc54ffcecf64e0f8e9c594d5978875b",
      "3fe0b562fd0945cc8a7ffac345e7f7e2",
      "a12910155b7e4bf48ce7040dc02d2314",
      "5c822990f483480c8ddbe7b4465c00ff",
      "478b5d153d0d45c58cb01152022bf6bf",
      "86203747c01f477ba25e411fb99f15d8",
      "5cea399ee7174ef48eae37d97b602b85",
      "7b28e69fd15f44fb965939114fbfe4fa",
      "093e055f8e134729a18f14b58f0d3827",
      "ce15a83814674cc697b2f165f6b0f95e",
      "0200419a2e2c4ef29fcde0b98d686a06",
      "0909fc8bd55c47fe8550af66b62591f7",
      "5595aa121b8e498ca405b659330e0a42",
      "9997f4ce16c24b6ebc09f5bb78e999e2",
      "5c95d60fe7224c2f9a62ddb6d08a8da5",
      "84c0fa190f9147668e0c775094ab1c8f",
      "f8f38f0d5161409b9bf96eb1f1e4075d",
      "521adffbc1034891b72de60637703801",
      "a8edf658b88746ea84725ac42bc917b8",
      "e1496a35ab1b49359acb592d07d8335c",
      "8118542ae8814644a01f0e804d38dc53",
      "a945ff55c11641dd9faa301742a20f98",
      "c886a4036fca44859cab0d6625103090",
      "87da7217190249859696bb353b1bfb70",
      "a1aed3e9213e4be681653b3b8aa59635",
      "43f17a250f2c4c88b0e125e4fa8fafeb",
      "6ee4c735f341494085bdf7c1b188b8d7",
      "618f9ec01c8845e8bc319faefa5637c9",
      "43f7dc49c6a74ac0be30656dc1bfe8cf",
      "bade27d608744fe7adedf1084c33fccf",
      "fbda9456e8bc4e1fb44b74c33968455d",
      "e18bbd58258e437eb947520857884086",
      "b90bbd050ca240f0883bdeb5f0ac7891",
      "a5f1e5b3940743b68a9daedcc3fce8cd",
      "f2d476056f7f471c988b909958c04c95",
      "abb5801a2f7740db9b241c0ce110182a",
      "56653d9256d540cc878672bb75a597a7",
      "388584e64958401691b6fdfc7df3509e",
      "c96e9c90e6fa432f856777dff1d45321",
      "7bda245e649d462b89eb40b73ff0ee91",
      "be95e10665ab4c33a15871936410d731",
      "e460282a6c4e4203ba34d86346b2b3d9",
      "c42844a77802418c9f6b90865a663b9e",
      "bc33815271a94c7e9c98cfe14affc8e5",
      "685ff78c62a54bf0af12d10eaf997c32",
      "536cffa4bcb84b1192236c21535a5e19",
      "7d1de2785f054ec9bffe8fa168aa7f5e",
      "992567ebb459455ebc03143ba7abed97",
      "95e4cbf718f54c829fbbf91f4cba3b59",
      "faa43e7c75e84300984c7201597d4642",
      "17446175f693498db593fb8e72ee94d1",
      "fd06207d71d84e1cbcdcbebc6198f6ce",
      "daca99493c2848cf8d6499877bc280c0",
      "1364c480517640ab88924fed82016a0b",
      "4634197399464a9c84755e4b3e567912",
      "2f62c19a0f464947b9fd04d579d642fc",
      "b712331a26914aee9c86e1707e6f01f8",
      "24fb90d301424e7b853d5847fc32c800",
      "6ef02d30a51848548f69a275263ea086",
      "ff9170ff3a37412c9ad41f2be5d5efec",
      "755630703da6492fb38ea1b831760812",
      "af7e75d5696b4cadb4fd6f8ae9ebba42",
      "1f5d3e5e4aac4dd8ab07a73b5685ede2",
      "f9f7705e9fcf482b95c812d54741415e",
      "3819841ba2554f1183a621a8825a373e",
      "5ece6a45cdca40f793c4e9ac1ecab002",
      "e49d0b223a9044838059b91aa67702a8",
      "81dbe3c5fc124fc382a510ba45a8afae",
      "9aefed70919f42bf8f98922d91a888df",
      "4058b483dc4c47329ce442dab70d2e50",
      "a74b149a8b4549229f35b763499eef3c",
      "e0d6d2c4a3f9430dab4e09af69356496",
      "a86475b35cee4e2da5d53bf0351d8b02",
      "c8004eec89e34e48b605e63f1475eba1",
      "258c9c4a6489411996b5469618d3bfb9",
      "7080b55c2f48481192016452f7dc3f17",
      "ea355521bf0a4d29af6e0cf34722d39f",
      "caeb46e45c204b8d84209dfb855a225f",
      "91b7cd013db647859fcf1a630fc4ff60",
      "baf222c00b7042a09c7fd6c351ff7a1b",
      "d9170514c20b47e9b8d693cc33f7f932",
      "53c43fda65794c1aa17a55aabace9d0c",
      "e33a81ce27084ae2a688f78e76647674",
      "a45ec705f16447c5819c2f264290cee1",
      "1b9351a3713e43c98b1f88fb4aa6a451",
      "048b4226de794da2afb35cae6bf81241",
      "99833643147848bfac2e75ca84b1d226",
      "10eceea86ee640cda397ade278b1a3b0",
      "b2a13c4e50e64af3b484b2b80212b4f4",
      "ee0340abb5834069a7c144bd50c7f405",
      "e00bf5e65f7846e1bf60c8661d4bd2de",
      "e1ff38195e3a414d9f8202de9609bb35",
      "a4d7a4c9f365497caa71bbc6e4fc9b8b",
      "a8dd49fae98e4b53aa0b34e0457784f6",
      "6ee342eda4c94e2ebf5b79154eb179aa",
      "ecb3f49cbcf94722b2dc70051f0110f9",
      "41bb7ee6a4314dec9a2f8ae281536eac"
     ]
    },
    "id": "2aoO9wDi1lGR",
    "outputId": "a564d822-cd72-4930-fe71-18547c07c493"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset, info = tfds.load(\n",
    "        'voc/2007',\n",
    "        data_dir='/root/tensorflow_datasets',  # Ensure this is the correct directory\n",
    "        download=True,  # Set this to True to allow downloading the dataset\n",
    "        split=['train', 'validation'],\n",
    "        with_info=True\n",
    "    )\n",
    "\n",
    "label_names = info.features['objects']['label'].names\n",
    "train_dataset, validation_dataset = dataset[0], dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "MitlfDG11YdH"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot raw images with their original label IDs\n",
    "def plot_raw_dataset(dataset, num_images=9):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, example in enumerate(dataset.take(num_images)):\n",
    "        image = example['image']\n",
    "        label_ids = example['objects']['label']\n",
    "\n",
    "        # Convert TensorFlow tensor to numpy and plot\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(image.numpy())\n",
    "        label_text = \", \".join(str(label_id.numpy()) for label_id in label_ids)\n",
    "        plt.title(f\"Raw labels: {label_text}\", fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "id": "iwCowSTA1yvu",
    "outputId": "12a4b583-6e16-4156-8f61-fe69fe753fcd"
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_raw_dataset(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "71ea046b"
   },
   "source": [
    "\n",
    "# Exploratory Data Visualization (Before Cleaning)\n",
    "\n",
    "## Why plot before preprocessing?\n",
    "\n",
    "Before applying any preprocessing (such as resizing, normalization, augmentation), it's crucial to **visualize the raw dataset** because:\n",
    "\n",
    "- It reveals **imbalances** among object classes (important for model fairness and performance).\n",
    "- It exposes **variation in image sizes**, helping to decide appropriate resizing strategies.\n",
    "- It helps detect **data issues** like extremely small or large images, missing labels, etc.\n",
    "- It justifies **why** certain preprocessing steps are needed (not arbitrary).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "id": "b4d2fc65",
    "outputId": "e785347f-aec5-4399-880b-c9607b0e8211"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot Class Label Distribution using actual train_dataset\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_names = info.features['objects']['label'].names\n",
    "label_counter = collections.Counter()\n",
    "\n",
    "for example in tfds.as_numpy(train_dataset):\n",
    "    for obj in example['objects']['label']:\n",
    "        label_counter[int(obj)] += 1\n",
    "\n",
    "labels = list(label_counter.keys())\n",
    "counts = list(label_counter.values())\n",
    "label_names_plot = [label_names[idx] for idx in labels]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.bar(label_names_plot, counts)\n",
    "plt.title('Class Label Distribution (Before Preprocessing)')\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "7e071257"
   },
   "source": [
    "\n",
    "## Class Label Distribution\n",
    "\n",
    "- This plot shows the number of training images available for each object class.\n",
    "- Highly **imbalanced classes** can bias the model predictions towards majority classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "afc1ea9d",
    "outputId": "0cb444c9-7a47-4939-c9d8-45c65f493f9c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot Image Size Distribution before resizing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_widths = []\n",
    "image_heights = []\n",
    "\n",
    "for example in tfds.as_numpy(train_dataset):\n",
    "    height, width, _ = example['image'].shape\n",
    "    image_widths.append(width)\n",
    "    image_heights.append(height)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(image_widths, image_heights, alpha=0.5)\n",
    "plt.title('Image Width vs Height Distribution (Before Resizing)')\n",
    "plt.xlabel('Width (pixels)')\n",
    "plt.ylabel('Height (pixels)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "f1b46da0"
   },
   "source": [
    "\n",
    "## Image Width vs Height Distribution\n",
    "\n",
    "- This scatter plot shows how image **widths** and **heights** vary across the dataset.\n",
    "- Real-world datasets like VOC2007 have **non-uniform image sizes**.\n",
    "- Therefore, **resizing images** to a common size (like 224×224) before feeding into CNNs is essential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "f19e2321"
   },
   "source": [
    "# Dataset Preprocessing with Image and Label Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "id": "cb8aeb7a"
   },
   "outputs": [],
   "source": [
    "# number of epochs, batch size, image size and Autotuning\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "IMAGE_SIZE = 224\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "#data preprocessing block\n",
    "def preprocess_data(example, label_names):\n",
    "\n",
    "    #image preprocessing sub-block below:-\n",
    "    image = example['image']\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "    image = (image / 127.5) - 1\n",
    "\n",
    "    #labels pre-processing below\n",
    "    labels = tf.zeros(len(label_names), dtype=tf.float32)\n",
    "    objects = example['objects']\n",
    "    label_ids = objects['label']\n",
    "\n",
    "    #here we are creating one-hot encoding tensor\n",
    "    for label_id in label_ids:\n",
    "        labels = tf.tensor_scatter_nd_update(\n",
    "            labels,\n",
    "            [[label_id]],\n",
    "            [1.0]\n",
    "        )\n",
    "\n",
    "    return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "id": "1iyfk3Np0CTB"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper: convert one-hot label vector back to class names\n",
    "def decode_one_hot_labels(one_hot_tensor, label_names):\n",
    "    indices = tf.where(one_hot_tensor == 1.0)\n",
    "    return [label_names[i[0].numpy()] for i in indices]\n",
    "\n",
    "# Plot preprocessed dataset\n",
    "def plot_preprocessed_dataset(dataset, label_names, num_images=9):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, (image, labels) in enumerate(dataset.take(num_images)):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        image = tf.squeeze(image)  # remove batch dimension if present\n",
    "        plt.imshow((image + 1) / 2)  # scale image from [-1, 1] back to [0, 1]\n",
    "        label_names_list = decode_one_hot_labels(labels, label_names)\n",
    "        plt.title(\", \".join(label_names_list), fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "IrBEOdXn2tyg",
    "outputId": "672cb02c-42b7-48e5-b131-cb3d7c2e615c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Apply preprocessing function to the dataset\n",
    "preprocessed_dataset = train_dataset.map(lambda x: preprocess_data(x, label_names))\n",
    "\n",
    "# Plot the results\n",
    "plot_preprocessed_dataset(preprocessed_dataset, label_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "02b0c0f5"
   },
   "source": [
    "# Data Augmentation for Improved Generalization and Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "id": "b89d8fbb"
   },
   "outputs": [],
   "source": [
    "# performs data augmentation on the input images to improve model generalization and robustness\n",
    "def apply_augmentation(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    image = tf.clip_by_value(image, -1, 1)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "b76IEaSz4186"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to show side-by-side comparison: original vs augmented\n",
    "def plot_original_vs_augmented(dataset, num_images=5):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, (image, label) in enumerate(dataset.take(num_images)):\n",
    "        # Original image\n",
    "        ax = plt.subplot(num_images, 2, 2 * i + 1)\n",
    "        plt.imshow(((image + 1) / 2).numpy())  # Convert from [-1, 1] to [0, 1]\n",
    "        plt.title(\"Original\", fontsize=10)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Augmented image\n",
    "        augmented_image, _ = apply_augmentation(image, label)\n",
    "        ax = plt.subplot(num_images, 2, 2 * i + 2)\n",
    "        plt.imshow(((augmented_image + 1) / 2).numpy())\n",
    "        plt.title(\"Augmented\", fontsize=10)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "NQL5LsUO5aID",
    "outputId": "66a2fe9d-c984-436a-9661-732d8018e7d9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Show before-and-after augmentation to demonstrate variation and robustness\n",
    "plot_original_vs_augmented(preprocessed_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "efb82479"
   },
   "source": [
    "# Dataset Preparation with Preprocessing, Augmentation, and Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "2ac4fe49"
   },
   "outputs": [],
   "source": [
    "#prepares proprocessing for images and labels in the below code and shuffles data as well\n",
    "def prepare_dataset(dataset, label_names, is_training=True):\n",
    "    dataset = dataset.map(lambda x: preprocess_data(x, label_names), num_parallel_calls=AUTOTUNE)\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(10000)\n",
    "        dataset = dataset.map(apply_augmentation, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "MVHnlZQ35y5R"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compare_dataset_pipeline_fixed(raw_ds, label_names, num_images=5):\n",
    "    # Collect a fixed list of raw examples\n",
    "    raw_examples = list(raw_ds.take(num_images))\n",
    "\n",
    "    plt.figure(figsize=(15, num_images * 3))\n",
    "\n",
    "    for i, raw_example in enumerate(raw_examples):\n",
    "        # --- Raw image and label ---\n",
    "        raw_image = raw_example['image']\n",
    "        raw_labels = raw_example['objects']['label']\n",
    "\n",
    "        # --- Preprocessed image and label ---\n",
    "        pre_image, pre_labels = preprocess_data(raw_example, label_names)\n",
    "\n",
    "        # --- Augmented image and label ---\n",
    "        aug_image, aug_labels = apply_augmentation(pre_image, pre_labels)\n",
    "\n",
    "        # --- Plot raw ---\n",
    "        ax = plt.subplot(num_images, 3, 3 * i + 1)\n",
    "        plt.imshow(raw_image.numpy())\n",
    "        label_ids = \", \".join(str(l.numpy()) for l in raw_labels)\n",
    "        plt.title(f\"Raw\\nLabels: {label_ids}\", fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # --- Plot preprocessed ---\n",
    "        ax = plt.subplot(num_images, 3, 3 * i + 2)\n",
    "        plt.imshow(((pre_image + 1) / 2).numpy())\n",
    "        pre_names = decode_one_hot_labels(pre_labels, label_names)\n",
    "        plt.title(\"Preprocessed\\n\" + \", \".join(pre_names), fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # --- Plot augmented ---\n",
    "        ax = plt.subplot(num_images, 3, 3 * i + 3)\n",
    "        plt.imshow(((aug_image + 1) / 2).numpy())\n",
    "        aug_names = decode_one_hot_labels(aug_labels, label_names)\n",
    "        plt.title(\"Prepared (Augmented)\\n\" + \", \".join(aug_names), fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 933
    },
    "id": "Jj1hod1R5zGR",
    "outputId": "c4ef6c97-66c2-44a4-9f8d-7652f888ac99"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compare same image through raw → preprocessed → augmented stages\n",
    "compare_dataset_pipeline_fixed(train_dataset, label_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "f52146c0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "id": "dLNADmO34bu_"
   },
   "outputs": [],
   "source": [
    "def label_preprocessing(image, labels):\n",
    "    # Resize to 224x224\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "id": "rwOOckpB4gbf"
   },
   "outputs": [],
   "source": [
    "def data_augmentation(image, labels):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    return image, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "id": "R5Xjzir_3d21"
   },
   "outputs": [],
   "source": [
    "# 1. Preprocessing\n",
    "train_dataset_processed = train_dataset.map(lambda x: label_preprocessing(x['image'], x['objects']['label']))\n",
    "\n",
    "# 2. Augmentation\n",
    "train_dataset_augmented = train_dataset_processed.map(lambda x, y: data_augmentation(x, y))\n",
    "\n",
    "# 3. No batch, no prefetch here!\n",
    "final_dataset_for_plotting = train_dataset_augmented\n",
    "\n",
    "# Batch and prefetch separately for training\n",
    "final_dataset = final_dataset_for_plotting.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "id": "UVgISk3Le2VN",
    "outputId": "8deb9c78-74a3-4698-ec89-99b995be70ba"
   },
   "outputs": [],
   "source": [
    "# After Preprocessing: Plot Label Distribution\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_counter_post = collections.Counter()\n",
    "\n",
    "# Loop through normalized, one-hot preprocessed dataset\n",
    "for image, labels in tfds.as_numpy(preprocessed_dataset):  # notice we use preprocessed_dataset now\n",
    "    label_indices = np.where(labels == 1)[0]  # Get indices where label is 1 (one-hot)\n",
    "    for idx in label_indices:\n",
    "        label_counter_post[idx] += 1\n",
    "\n",
    "# Prepare plotting\n",
    "labels_post = list(label_counter_post.keys())\n",
    "counts_post = list(label_counter_post.values())\n",
    "label_names_plot_post = [label_names[idx] for idx in labels_post]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.bar(label_names_plot_post, counts_post, color='skyblue')\n",
    "plt.title('Class Label Distribution (After Normalization and One-Hot Encoding)')\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "id": "VwooUDWdLMN7"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "Fs5bAef-19kT",
    "outputId": "eb263a6f-4b1d-4539-dec3-d27cbdd3af39"
   },
   "outputs": [],
   "source": [
    "image_widths_post = []\n",
    "image_heights_post = []\n",
    "\n",
    "# Limit to first 500 samples for plotting\n",
    "for image, labels in tfds.as_numpy(final_dataset_for_plotting):\n",
    "    height, width, _ = image.shape\n",
    "    image_widths_post.append(width)\n",
    "    image_heights_post.append(height)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(image_widths_post, image_heights_post, alpha=0.5, color='orange')\n",
    "plt.title('Image Width vs Height Distribution (After Resizing)')\n",
    "plt.xlabel('Width (pixels)')\n",
    "plt.ylabel('Height (pixels)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "YAeJsQANLavW"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cf8ee80",
    "outputId": "aee8972a-4523-4724-dda2-92e120fdb187"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    output_dir = 'model_output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load dataset and retrieve label names dynamically, set download=True to allow TFDS to download\n",
    "    dataset, info = tfds.load(\n",
    "        'voc/2007',\n",
    "        data_dir='/root/tensorflow_datasets',  # Ensure this is the correct directory\n",
    "        download=True,  # Set this to True to allow downloading the dataset\n",
    "        split=['train', 'validation'],\n",
    "        with_info=True\n",
    "    )\n",
    "\n",
    "    label_names = info.features['objects']['label'].names\n",
    "    train_dataset, validation_dataset = dataset[0], dataset[1]\n",
    "\n",
    "    # Prepare the datasets\n",
    "    train_dataset = prepare_dataset(train_dataset, label_names, is_training=True)\n",
    "    validation_dataset = prepare_dataset(validation_dataset, label_names, is_training=False)\n",
    "\n",
    "    print(\"Creating model...\")\n",
    "    model = MobileNetV2(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), num_classes=len(label_names))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "            tf.keras.metrics.AUC(name='auc'),\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            os.path.join(output_dir, 'mobilenetv2_pascal_best.keras'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=7,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=2,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    print(\"\\nStarting training...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.save(os.path.join(output_dir, 'mobilenetv2_pascal_final.keras'))\n",
    "\n",
    "    return history, train_dataset, label_names\n",
    "\n",
    "\n",
    "# Run the main function with mixed precision\n",
    "if __name__ == \"__main__\":\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    history, train_dataset, label_names = main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "id": "BeHr_L9B0W1Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "f15ce052"
   },
   "source": [
    "# Model Predictions with Actual Labels function:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "id": "dd89df0f"
   },
   "outputs": [],
   "source": [
    "def display_more_predictions(model, dataset, label_names, num_images=15, threshold=0.3):\n",
    "    # Initialize counter for displayed images\n",
    "    displayed_count = 0\n",
    "\n",
    "    # Loop through dataset batches\n",
    "    for images, labels in dataset:\n",
    "        predictions = model.predict(images)\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            if displayed_count >= num_images:\n",
    "                return\n",
    "\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.imshow((images[i] + 1) / 2)\n",
    "            plt.axis('off')\n",
    "\n",
    "            # show actual labels and the predictions:-\n",
    "            actual_labels = [label_names[j] for j in range(len(label_names)) if labels[i][j] == 1]\n",
    "            predicted_labels = [label_names[j] for j in range(len(label_names)) if predictions[i][j] > threshold]\n",
    "\n",
    "            # when no labels is able to meet the model predictions:-\n",
    "            if not predicted_labels:\n",
    "                predicted_labels = [\"No confident prediction\"]\n",
    "\n",
    "            plt.title(f\"Actual: {', '.join(actual_labels)}\\nPredicted: {', '.join(predicted_labels)}\")\n",
    "            plt.show()\n",
    "\n",
    "            displayed_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "b4e560fe"
   },
   "source": [
    "# Test Dataset Preparation and Display the Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4a697b26",
    "outputId": "63b5fada-ca48-42d9-f628-94817d9af7a3"
   },
   "outputs": [],
   "source": [
    "# Load and prepare the test dataset\n",
    "test_dataset, info = tfds.load(\n",
    "    'voc/2007',\n",
    "    data_dir='/root/tensorflow_datasets',\n",
    "    download=False,\n",
    "    split='test',\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "# here we are getting the label names\n",
    "label_names = info.features['objects']['label'].names\n",
    "\n",
    "# preparartion of the test dataset\n",
    "test_dataset = prepare_dataset(test_dataset, label_names=label_names, is_training=False)\n",
    "\n",
    "# Loading the trained model to test o test dataset\n",
    "model_path = '/content/model_output/mobilenetv2_pascal_best.keras'\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Display predictions on more test images\n",
    "display_more_predictions(model, test_dataset, label_names=label_names, num_images=15, threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "id": "c7f837d2"
   },
   "source": [
    "# Evaluation of Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "id": "527e312d"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# test dataset labels\n",
    "label_names = info.features['objects']['label'].names\n",
    "\n",
    "# Evaluation metrics\n",
    "def evaluate_metrics_with_progress_bar(model, dataset, label_names, threshold=0.5):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_scores = []\n",
    "    accuracies = []\n",
    "\n",
    "    # Create a progress bar\n",
    "    for images, labels in tqdm(dataset, desc=\"Evaluating metrics\", unit=\"batch\"):\n",
    "        # Get model predictions\n",
    "        predictions = model(images, training=False)\n",
    "\n",
    "        # threshold for predictions\n",
    "        predictions = tf.cast(predictions > threshold, tf.int32)\n",
    "\n",
    "        # accuracy for the batch\n",
    "        batch_accuracy = np.mean(np.equal(np.argmax(predictions, axis=-1), np.argmax(labels, axis=-1)))\n",
    "        accuracies.append(batch_accuracy)\n",
    "\n",
    "        y_true.append(labels.numpy())\n",
    "        y_pred.append(predictions.numpy())\n",
    "        y_scores.append(predictions.numpy())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    y_scores = np.concatenate(y_scores, axis=0)\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=label_names, output_dict=True, zero_division=1)\n",
    "\n",
    "    # Compute AUC value for each class\n",
    "    auc_scores = []\n",
    "    for i in range(len(label_names)):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i]) if y_true.shape[1] > i else None\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    return report, auc_scores, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "id": "158ceae5"
   },
   "source": [
    "# Generate Model Evaluation Report and AUC Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96ebcfcd",
    "outputId": "05b7d6d8-e25e-4ecf-ae42-8c857e3ad26c"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Call the function\n",
    "report, auc_scores, accuracies = evaluate_metrics_with_progress_bar(model, test_dataset, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {
    "id": "0eda7e5d"
   },
   "source": [
    "# Display plots for Precision, Recall and F-1 Score per class:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "5a5e13ea",
    "outputId": "83a6e7ef-a5cf-4f6d-9d70-38b27282f108"
   },
   "outputs": [],
   "source": [
    "# function for classification report\n",
    "def plot_classification_report(report, label_names):\n",
    "    metrics = ['precision', 'recall', 'f1-score']\n",
    "    data = {metric: [] for metric in metrics}\n",
    "\n",
    "    for class_name in label_names:\n",
    "        for metric in metrics:\n",
    "            data[metric].append(report[class_name][metric])\n",
    "\n",
    "    # Plots for precision, recall and F-1 Score:-\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(18, 5))\n",
    "    for i, metric in enumerate(metrics):\n",
    "        axes[i].bar(label_names, data[metric], color='skyblue')\n",
    "        axes[i].set_title(f'{metric.capitalize()} per Class')\n",
    "        axes[i].set_xticks(range(len(label_names)))\n",
    "        axes[i].set_xticklabels(label_names, rotation=45, ha=\"right\")\n",
    "        axes[i].set_xlabel('Classes')\n",
    "        axes[i].set_ylabel(f'{metric.capitalize()}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_classification_report(report, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "id": "e04301d0"
   },
   "source": [
    "# Visualization of AUC Scores for Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "dcb3951e",
    "outputId": "3557479c-1e3e-4407-be1f-02072c129e98"
   },
   "outputs": [],
   "source": [
    "def plot_auc_scores(auc_scores, label_names):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(label_names, auc_scores, color='lightcoral')\n",
    "    plt.title('AUC Scores per Class')\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_auc_scores(auc_scores, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {
    "id": "bd4f5794"
   },
   "source": [
    "# Plotting Model Accuracy per Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "b2cd898c",
    "outputId": "a732af1f-63c4-4af8-b81d-c03d8bfc55e3"
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(accuracies):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(accuracies, label='Accuracy per Batch', color='green')\n",
    "    plt.title('Model Accuracy per Batch')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "id": "e06c0e41"
   },
   "source": [
    "# Precision, Recall, F-1 and AUC scores for all classes:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33cf9e35",
    "outputId": "544098ba-0c13-4d5f-94cb-851f2378d0e0"
   },
   "outputs": [],
   "source": [
    "# Print the final outputs\n",
    "print(\"\\nFinal Classification Report:\")\n",
    "for class_name, metrics in report.items():\n",
    "    print(f\"{class_name}: {metrics}\")\n",
    "\n",
    "print(\"\\nAUC Scores:\")\n",
    "for idx, auc in enumerate(auc_scores):\n",
    "    if auc is not None:\n",
    "        print(f\"AUC for {label_names[idx]}: {auc:.4f}\")\n",
    "    else:\n",
    "        print(f\"AUC for {label_names[idx]}: Not computable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {
    "id": "9a80c9f6"
   },
   "source": [
    "# Mean Intersection over Union (mIOU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f4fc5b7",
    "outputId": "a8c151ba-1ba6-4e7a-d0db-fbe9ef99ed81"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Function to calculate mIOU for the model with a progress bar\n",
    "def calculate_mean_iou(model, dataset, num_classes):\n",
    "    # Initialize MeanIoU metric\n",
    "    miou_metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    # Create a progress bar\n",
    "    dataset = tqdm(dataset, desc=\"Processing batches\", unit=\"batch\")\n",
    "\n",
    "    # Iterate over test dataset\n",
    "    for images, labels in dataset:\n",
    "        predictions = model.predict(images, verbose=0)\n",
    "        predicted_labels = tf.argmax(predictions, axis=-1) #predicted lables\n",
    "        true_labels = tf.argmax(labels, axis=-1) #true labels\n",
    "\n",
    "        # Update the mIOU metric with the true and predicted labels\n",
    "        miou_metric.update_state(true_labels, predicted_labels)\n",
    "\n",
    "    # final #mIOU score\n",
    "    mean_iou = miou_metric.result().numpy()\n",
    "    print(f\"Mean Intersection over Union (mIOU): {mean_iou:.4f}\")\n",
    "    return mean_iou\n",
    "\n",
    "# Load the test dataset\n",
    "num_classes = 20\n",
    "mean_iou = calculate_mean_iou(model, test_dataset, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {
    "id": "78658ec5"
   },
   "source": [
    "# Calculating Mean Intersection over Union (mIOU) and Per-Class mIOU values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dedfbd6",
    "outputId": "63db7a66-186e-4f50-f387-cdc7c7b7126c"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# mIOU per class\n",
    "def calculate_mean_iou_per_class(model, dataset, num_classes):\n",
    "    # Initialize MeanIoU metric\n",
    "    confusion_matrix = tf.zeros((num_classes, num_classes), dtype=tf.int32)\n",
    "    dataset = tqdm(dataset, desc=\"Processing batches\", unit=\"batch\")\n",
    "\n",
    "    # loop over test dataset\n",
    "    for images, labels in dataset:\n",
    "        predictions = model.predict(images, verbose=0)\n",
    "        predicted_labels = tf.argmax(predictions, axis=-1)\n",
    "        true_labels = tf.argmax(labels, axis=-1)\n",
    "\n",
    "        # Update the confusion matrix\n",
    "        for true, pred in zip(tf.reshape(true_labels, [-1]), tf.reshape(predicted_labels, [-1])):\n",
    "            if true < num_classes and pred < num_classes:  # Ensure labels are valid\n",
    "                confusion_matrix = tf.tensor_scatter_nd_add(\n",
    "                    confusion_matrix, [[true, pred]], [1]\n",
    "                )\n",
    "\n",
    "    # mIOU per class\n",
    "    intersection = tf.linalg.diag_part(confusion_matrix)\n",
    "    union = tf.reduce_sum(confusion_matrix, axis=0) + tf.reduce_sum(confusion_matrix, axis=1) - intersection\n",
    "    iou_per_class = intersection / tf.maximum(union, 1)\n",
    "\n",
    "    # Mean mIOU across classes\n",
    "    mean_iou = tf.reduce_mean(iou_per_class).numpy()\n",
    "\n",
    "    print(f\"Mean Intersection over Union (mIOU): {mean_iou:.4f}\")\n",
    "    print(\"Per-Class mIOU:\")\n",
    "    for i, iou in enumerate(iou_per_class.numpy()):\n",
    "        print(f\"{class_names[i]}: mIOU = {iou:.4f}\")\n",
    "\n",
    "    return mean_iou, iou_per_class.numpy()\n",
    "\n",
    "\n",
    "# 20 classes\n",
    "num_classes = 20\n",
    "class_names = [\n",
    "    \"Aeroplane\", \"Bicycle\", \"Bird\", \"Boat\", \"Bottle\", \"Bus\", \"Car\", \"Cat\", \"Chair\", \"Cow\",\n",
    "    \"Dining Table\", \"Dog\", \"Horse\", \"Motorbike\", \"Person\", \"Potted Plant\", \"Sheep\", \"Sofa\", \"Train\", \"TV Monitor\"\n",
    "]\n",
    "\n",
    "mean_iou, iou_per_class = calculate_mean_iou_per_class(model, test_dataset, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {
    "id": "f2ae9b3a"
   },
   "source": [
    "# Plotting Per-Class Mean Intersection over Union (mIOU) Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "43ea4432",
    "outputId": "7ebea3ac-3556-4528-b559-dc662bec43f8"
   },
   "outputs": [],
   "source": [
    "# plot per class mIOU scores\n",
    "def plot_per_class_miou(iou_per_class, class_names):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(class_names, iou_per_class, color=\"blue\")\n",
    "    plt.title(\"Per-Class Mean Intersection over Union (mIOU)\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"mIOU Score\")\n",
    "    plt.xlabel(\"Classes\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the per-class mIOU scores\n",
    "plot_per_class_miou(iou_per_class, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "id": "e60cb6e0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "id": "d99dc132"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "id": "f2c2d33b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
